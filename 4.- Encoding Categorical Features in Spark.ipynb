{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categorical features in Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- String Indexing\n",
    "String Indexer encodes a column of string labels/categories to a column of indices. The ordering of the indices is done on the basis of popularity and the range is [0, numOfLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-FQ2BOOJ:4040\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1557763667635)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: int, category: string]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.createDataFrame(Seq((0, \"apples\"), (1, \"oranges\"), (2, \"banana\"), (3, \"apples\"), (4, \"banana\"), (5, \"oranges\"), (6, \"apples\"))).toDF(\"id\", \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|  apples|\n",
      "|  1| oranges|\n",
      "|  2|  banana|\n",
      "|  3|  apples|\n",
      "|  4|  banana|\n",
      "|  5| oranges|\n",
      "|  6|  apples|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+\n",
      "| id|category|category_index|\n",
      "+---+--------+--------------+\n",
      "|  0|  apples|           0.0|\n",
      "|  1| oranges|           1.0|\n",
      "|  2|  banana|           2.0|\n",
      "|  3|  apples|           0.0|\n",
      "|  4|  banana|           2.0|\n",
      "|  5| oranges|           1.0|\n",
      "|  6|  apples|           0.0|\n",
      "+---+--------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_71614f945471\n",
       "df_indexed1: Unit = ()\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val indexer = new StringIndexer().setInputCol(\"category\").setOutputCol(\"category_index\")\n",
    "\n",
    "val df_indexed1 = indexer.fit(df).transform(df).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another column to our toy dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: int, category: string ... 1 more field]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.createDataFrame(Seq((0, \"apples\", \"red\"), (1, \"oranges\", \"orange\"), (2, \"banana\", \"yellow\"), (3, \"apples\", \"red\"), (4, \"banana\", \"yellow\"), (5, \"oranges\", \"orange\"), \n",
    "                                   (6, \"apples\", \"red\"))).toDF(\"id\", \"category\", \"colour\")                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "| id|category|colour|\n",
      "+---+--------+------+\n",
      "|  0|  apples|   red|\n",
      "|  1| oranges|orange|\n",
      "|  2|  banana|yellow|\n",
      "|  3|  apples|   red|\n",
      "|  4|  banana|yellow|\n",
      "|  5| oranges|orange|\n",
      "|  6|  apples|   red|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+--------------+------------+\n",
      "| id|category|colour|category_index|colour_index|\n",
      "+---+--------+------+--------------+------------+\n",
      "|  0|  apples|   red|           0.0|         0.0|\n",
      "|  1| oranges|orange|           1.0|         1.0|\n",
      "|  2|  banana|yellow|           2.0|         2.0|\n",
      "|  3|  apples|   red|           0.0|         0.0|\n",
      "|  4|  banana|yellow|           2.0|         2.0|\n",
      "|  5| oranges|orange|           1.0|         1.0|\n",
      "|  6|  apples|   red|           0.0|         0.0|\n",
      "+---+--------+------+--------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "features: Array[String] = Array(category, colour)\n",
       "encodedFeatures: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_c95aa955927b, strIdx_a190bc56261e)\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_1eb6bbcdc149\n",
       "indexer_model: org.apache.spark.ml.PipelineModel = pipeline_1eb6bbcdc149\n",
       "df_transformed: org.apache.spark.sql.DataFrame = [id: int, category: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val features = df.columns.filterNot(_.contains(\"id\"))\n",
    "\n",
    "// here's how we encode several categorical features at once\n",
    "val encodedFeatures = features.flatMap{name =>\n",
    "    \n",
    "    val indexer = new StringIndexer().setInputCol(name).setOutputCol(name + \"_index\").setHandleInvalid(\"keep\")\n",
    "    Array(indexer)\n",
    "    \n",
    "}\n",
    "\n",
    "val pipeline = new Pipeline().setStages(encodedFeatures)\n",
    "\n",
    "val indexer_model = pipeline.fit(df)\n",
    "\n",
    "val df_transformed = indexer_model.transform(df)\n",
    "\n",
    "df_transformed.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- One-Hot Encoding\n",
    "\n",
    "This method is generally used when we need to use categorical features but the algorithm expects continuous features (e.g. regression). The Spark one hot encoder takes the indexed label/category from the string indexer and then encodes it into a sparse vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+--------------+-------------+------------+-------------+\n",
      "| id|category|colour|category_index| category_vec|colour_index|   colour_vec|\n",
      "+---+--------+------+--------------+-------------+------------+-------------+\n",
      "|  0|  apples|   red|           0.0|(4,[0],[1.0])|         0.0|(4,[0],[1.0])|\n",
      "|  1| oranges|orange|           1.0|(4,[1],[1.0])|         1.0|(4,[1],[1.0])|\n",
      "|  2|  banana|yellow|           2.0|(4,[2],[1.0])|         2.0|(4,[2],[1.0])|\n",
      "|  3|  apples|   red|           0.0|(4,[0],[1.0])|         0.0|(4,[0],[1.0])|\n",
      "|  4|  banana|yellow|           2.0|(4,[2],[1.0])|         2.0|(4,[2],[1.0])|\n",
      "|  5| oranges|orange|           1.0|(4,[1],[1.0])|         1.0|(4,[1],[1.0])|\n",
      "|  6|  apples|   red|           0.0|(4,[0],[1.0])|         0.0|(4,[0],[1.0])|\n",
      "+---+--------+------+--------------+-------------+------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
       "features: Array[String] = Array(category, colour)\n",
       "encodedFeatures: Array[org.apache.spark.ml.Estimator[_ >: org.apache.spark.ml.feature.OneHotEncoderModel with org.apache.spark.ml.feature.StringIndexerModel <: org.apache.spark.ml.Model[_ >: org.apache.spark.ml.feature.OneHotEncoderModel with org.apache.spark.ml.feature.StringIndexerModel <: org.apache.spark.ml.Transformer with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "\n",
    "val features = df.columns.filterNot(_.contains(\"id\"))\n",
    "\n",
    "val encodedFeatures = features.flatMap{name =>\n",
    "    \n",
    "    val indexer = new StringIndexer().setInputCol(name).setOutputCol(name + \"_index\").setHandleInvalid(\"keep\")\n",
    "    \n",
    "    val oneHotEncoder = new OneHotEncoderEstimator()\n",
    "         .setInputCols(Array(name + \"_index\"))\n",
    "         .setOutputCols(Array(name + \"_vec\"))\n",
    "         .setDropLast(false)\n",
    "    \n",
    "    Array(indexer, oneHotEncoder)\n",
    "    \n",
    "}\n",
    "\n",
    "val pipeline = new Pipeline().setStages(encodedFeatures)\n",
    "\n",
    "val indexer_model = pipeline.fit(df)\n",
    "\n",
    "val df_transformed = indexer_model.transform(df)\n",
    "\n",
    "df_transformed.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse vectors seen in the above dataframe have 4 different components. The first component which is a 0 indicates that it is a sparse vector. The second component talks about the size of the vector. The third component talks about the indices where the vector is populated while the fourth component talks about what values these are. This truncates the vector and is really efficient when you have really large vector representations. This truncates the vector and is really efficient when you have really large vector representations.\n",
    "\n",
    "It's fairly simple to see the dense vector representation for the above sparse vectors using a udf: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+--------------+-------------+------------+-------------+------------------+\n",
      "| id|category|colour|category_index| category_vec|colour_index|   colour_vec|dense_category_vec|\n",
      "+---+--------+------+--------------+-------------+------------+-------------+------------------+\n",
      "|  0|  apples|   red|           0.0|(4,[0],[1.0])|         0.0|(4,[0],[1.0])| [1.0,0.0,0.0,0.0]|\n",
      "|  1| oranges|orange|           1.0|(4,[1],[1.0])|         1.0|(4,[1],[1.0])| [0.0,1.0,0.0,0.0]|\n",
      "|  2|  banana|yellow|           2.0|(4,[2],[1.0])|         2.0|(4,[2],[1.0])| [0.0,0.0,1.0,0.0]|\n",
      "|  3|  apples|   red|           0.0|(4,[0],[1.0])|         0.0|(4,[0],[1.0])| [1.0,0.0,0.0,0.0]|\n",
      "|  4|  banana|yellow|           2.0|(4,[2],[1.0])|         2.0|(4,[2],[1.0])| [0.0,0.0,1.0,0.0]|\n",
      "|  5| oranges|orange|           1.0|(4,[1],[1.0])|         1.0|(4,[1],[1.0])| [0.0,1.0,0.0,0.0]|\n",
      "|  6|  apples|   red|           0.0|(4,[0],[1.0])|         0.0|(4,[0],[1.0])| [1.0,0.0,0.0,0.0]|\n",
      "+---+--------+------+--------------+-------------+------------+-------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "sparseToDense: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,Some(List(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7)))\n",
       "df_denseVectors: org.apache.spark.sql.DataFrame = [id: int, category: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "\n",
    "val sparseToDense = udf((v: Vector) => v.toDense)\n",
    "\n",
    "val df_denseVectors = df_transformed.withColumn(\"dense_category_vec\", sparseToDense($\"category_vec\")) \n",
    "\n",
    "df_denseVectors.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Vector Assembler\n",
    "\n",
    "Most machine learning algorithms in Spark expect a single encoded numerical vector as the input. In order to do that, we use something called the vector assembler. Its job is to combine the raw features and features generated from various transforms into a single feature vector. It accepts boolean, numerical and vector type inputs. Lets modify our earlier code to combine the category and the color vectors to a single feature vector.\n",
    "\n",
    "Vector assembler generally will always feature in your workflow when you want to combine all your features before training or scoring your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+--------------+-------------+------------+-------------+-------------------+\n",
      "| id|category|colour|category_index| category_vec|colour_index|   colour_vec|           features|\n",
      "+---+--------+------+--------------+-------------+------------+-------------+-------------------+\n",
      "|  0|  apples|   red|           0.0|(4,[0],[1.0])|         0.0|(4,[0],[1.0])|(8,[0,4],[1.0,1.0])|\n",
      "|  1| oranges|orange|           1.0|(4,[1],[1.0])|         1.0|(4,[1],[1.0])|(8,[1,5],[1.0,1.0])|\n",
      "|  2|  banana|yellow|           2.0|(4,[2],[1.0])|         2.0|(4,[2],[1.0])|(8,[2,6],[1.0,1.0])|\n",
      "|  3|  apples|   red|           0.0|(4,[0],[1.0])|         0.0|(4,[0],[1.0])|(8,[0,4],[1.0,1.0])|\n",
      "|  4|  banana|yellow|           2.0|(4,[2],[1.0])|         2.0|(4,[2],[1.0])|(8,[2,6],[1.0,1.0])|\n",
      "|  5| oranges|orange|           1.0|(4,[1],[1.0])|         1.0|(4,[1],[1.0])|(8,[1,5],[1.0,1.0])|\n",
      "|  6|  apples|   red|           0.0|(4,[0],[1.0])|         0.0|(4,[0],[1.0])|(8,[0,4],[1.0,1.0])|\n",
      "+---+--------+------+--------------+-------------+------------+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "features: Array[String] = Array(category, colour)\n",
       "encodedFeatures: Array[org.apache.spark.ml.Estimator[_ >: org.apache.spark.ml.feature.OneHotEncoderModel with org.apache.spark.ml.feature.StringIndexerModel <: org.apache.spark.ml.Model[_ >: org.apache.spark.ml.feature.OneHotEncoderModel with org.apache.spark.ml.feature.StringIndexerModel <: org.apache.spark.ml.Transformer with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark...."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val features = df.columns.filterNot(_.contains(\"id\"))\n",
    "\n",
    "val encodedFeatures = features.flatMap{name =>\n",
    "    \n",
    "    val indexer = new StringIndexer().setInputCol(name).setOutputCol(name + \"_index\").setHandleInvalid(\"keep\")\n",
    "    \n",
    "    val oneHotEncoder = new OneHotEncoderEstimator()\n",
    "         .setInputCols(Array(name + \"_index\"))\n",
    "         .setOutputCols(Array(name + \"_vec\"))\n",
    "         .setDropLast(false)\n",
    "    \n",
    "    Array(indexer, oneHotEncoder)\n",
    "    \n",
    "}\n",
    "\n",
    "val pipeline = new Pipeline().setStages(encodedFeatures)\n",
    "\n",
    "val indexer_model = pipeline.fit(df)\n",
    "\n",
    "val df_transformed = indexer_model.transform(df)\n",
    "\n",
    "val vecFeatures = df_transformed.columns.filter(_.contains(\"vec\")).toArray\n",
    "\n",
    "val vectorAssembler = new VectorAssembler().setInputCols(vecFeatures).setOutputCol(\"features\")\n",
    "    \n",
    "val pipelineVectorAssembler = new Pipeline().setStages(Array(vectorAssembler))\n",
    "\n",
    "val result_df = pipelineVectorAssembler.fit(df_transformed).transform(df_transformed)\n",
    "\n",
    "result_df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.- Vector Indexer\n",
    "\n",
    "Vector indexer allows us to skip the one hot encoding stage when encoding categorical features. The algorithm performs an inference based on the values of the features, automatically generating the desired feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+--------------+------------+------------+--------------------+\n",
      "| id|category|colour|category_index|colour_index|features_bis|indexed_features_bis|\n",
      "+---+--------+------+--------------+------------+------------+--------------------+\n",
      "|  0|  apples|   red|           0.0|         0.0|   (2,[],[])|           (2,[],[])|\n",
      "|  1| oranges|orange|           1.0|         1.0|   [1.0,1.0]|           [1.0,1.0]|\n",
      "|  2|  banana|yellow|           2.0|         2.0|   [2.0,2.0]|           [2.0,2.0]|\n",
      "|  3|  apples|   red|           0.0|         0.0|   (2,[],[])|           (2,[],[])|\n",
      "|  4|  banana|yellow|           2.0|         2.0|   [2.0,2.0]|           [2.0,2.0]|\n",
      "|  5| oranges|orange|           1.0|         1.0|   [1.0,1.0]|           [1.0,1.0]|\n",
      "|  6|  apples|   red|           0.0|         0.0|   (2,[],[])|           (2,[],[])|\n",
      "+---+--------+------+--------------+------------+------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorIndexer\n",
       "features: Array[String] = Array(category, colour)\n",
       "encodedFeatures: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_8dd7346f7fc3, strIdx_3311e22d1bb1)\n",
       "pipeline1: org.apache.spark.ml.Pipeline = pipeline_ffad86ceb189\n",
       "indexer_model1: org.apache.spark.ml.PipelineModel = pipeline_ffad86ceb189\n",
       "df_transformed_bis: org.apache.spark.sql.DataFrame = [id: int, category: string ... 3 more fields]\n",
       "features_bis: Array[String] = Array(category_index, colour_index)\n",
       "vectorAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_2a2a13c0b904\n",
       "vectorIndexer: org.apache.spark.ml.feature.VectorIndexer = vecIdx_ce46856b8a25\n",
       "pipeline2: org.apache.spark.ml.Pipeline = pipeline_f4a8043f4eb1\n",
       "indexer_model2: org.apache.spark.ml.PipelineMod..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorIndexer\n",
    "\n",
    "val features = df.columns.filterNot(_.contains(\"id\"))\n",
    "\n",
    "// no hot-one encoding this time\n",
    "val encodedFeatures = features.flatMap{name =>\n",
    "    \n",
    "    val indexer = new StringIndexer().setInputCol(name).setOutputCol(name + \"_index\").setHandleInvalid(\"keep\")\n",
    "    \n",
    "    Array(indexer)\n",
    "    \n",
    "}\n",
    "\n",
    "val pipeline1 = new Pipeline().setStages(encodedFeatures)\n",
    "\n",
    "val indexer_model1 = pipeline1.fit(df)\n",
    "\n",
    "val df_transformed_bis = indexer_model1.transform(df)\n",
    "\n",
    "//////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "// we take the string indexed features\n",
    "val features_bis = df_transformed_bis.columns.filter(_.contains(\"index\")).toArray\n",
    "\n",
    "// assembler\n",
    "val vectorAssembler = new VectorAssembler().setInputCols(features_bis).setOutputCol(\"features_bis\")\n",
    "\n",
    "// vector indexer\n",
    "val vectorIndexer = new VectorIndexer().setInputCol(\"features_bis\").setOutputCol(\"indexed_features_bis\").setMaxCategories(2)\n",
    "\n",
    "// adding to the pipeline\n",
    "val pipeline2 = new Pipeline().setStages(Array(vectorAssembler, vectorIndexer))\n",
    "\n",
    "val indexer_model2 = pipeline2.fit(df_transformed_bis)\n",
    "\n",
    "val result2_df = indexer_model2.transform(df_transformed_bis)\n",
    "\n",
    "result2_df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column \"features_bis\" is the output of the assembler but the categorical features are not encoded yet. The vector indexer will do this, producing \"indexed_features_bis\" as the final feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
