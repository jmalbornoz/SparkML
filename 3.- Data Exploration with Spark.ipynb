{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration with Spark\n",
    "\n",
    "Dr Jose M. Albornoz, April 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook a few data exploration techniques will be examined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.192:4040\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1556212296140)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.Row\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawDF: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawDF = spark.read.format(\"csv\").option(\"delimiter\",\" \").option(\"inferSchema\",\"true\").load(\"Data/LifeExpectancy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Long = 197\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawDF.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+----+----+\n",
      "|        _c0| _c1|   _c2| _c3| _c4|\n",
      "+-----------+----+------+----+----+\n",
      "|Afghanistan|null|48.673|null| SAs|\n",
      "|    Albania|null|76.918|null|EuCA|\n",
      "|    Algeria|null|73.131|null|MENA|\n",
      "|     Angola|null|51.093|null| SSA|\n",
      "|  Argentina|null|75.901|null|Amer|\n",
      "+-----------+----+------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(Country,StringType,true), StructField(LifeExp,DoubleType,true), StructField(Region,StringType,true))\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = StructType(Array(\n",
    "       StructField(\"Country\",StringType),\n",
    "       StructField(\"LifeExp\",DoubleType),\n",
    "       StructField(\"Region\",StringType)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectedDF: org.apache.spark.sql.DataFrame = [_c0: string, _c2: double ... 1 more field]\n",
       "lifeExpectancyDF: org.apache.spark.sql.DataFrame = [Country: string, LifeExp: double ... 1 more field]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val selectedDF = rawDF.select(\"_c0\",\"_c2\",\"_c4\")\n",
    "val lifeExpectancyDF = spark.createDataFrame(selectedDF.rdd,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+\n",
      "|    Country|LifeExp|Region|\n",
      "+-----------+-------+------+\n",
      "|Afghanistan| 48.673|   SAs|\n",
      "|    Albania| 76.918|  EuCA|\n",
      "|    Algeria| 73.131|  MENA|\n",
      "|     Angola| 51.093|   SSA|\n",
      "|  Argentina| 75.901|  Amer|\n",
      "+-----------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lifeExpectancyDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Five numbers summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Five Number Summary Contains refers to the following statistics:\n",
    "\n",
    "* Min - Minimum value of the column\n",
    "* First Quantile - 25% percentile of the column\n",
    "* Median\n",
    "* Third Quartile - 75%th percentile of the column\n",
    "* Max - maximum value of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|          LifeExp|\n",
      "+-------+-----------------+\n",
      "|  count|              197|\n",
      "|   mean|69.86281725888323|\n",
      "| stddev|9.668736205594511|\n",
      "|    min|           47.794|\n",
      "|    max|           83.394|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lifeExpectancyDF.describe(\"LifeExp\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calculating quantiles values and median, Spark's *describe* computes mean and standard deviation. The reason for this is that median and quantiles are costly to compute on large data; the data must be sorted order and the result of the calculation is skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate quartiles using the *approxQuantile* method introduced in Spark 2.0. The name of the method suggests that it provides approximate values, whose precision depend on a specified error rate; this makes calculation faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medianAndQuantiles: Array[Double] = Array(64.666, 73.235, 76.652)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val medianAndQuantiles = lifeExpectancyDF.stat.approxQuantile(\"LifeExp\", Array(0.25, 0.5, 0.75),0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's Dataframe API doesnâ€™t have a built-in function for histograms; however such a functionality exists in the RDD API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startValues: Array[Double] = Array(47.794, 54.914, 62.034, 69.154, 76.274, 83.394)\n",
       "counts: Array[Long] = Array(24, 18, 32, 69, 54)\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (startValues, counts) = lifeExpectancyDF.select(\"LifeExp\").map(value => value.getDouble(0)).rdd.histogram(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the histogram is two arrays:\n",
    "\n",
    "* The first array contains the starting values of each bin\n",
    "* The second array contains the count for each bin\n",
    "    \n",
    "We can thus see, for example, that there are 24 countries with a life expectancy between 47.8 and 54.9 years.  \n",
    "\n",
    "Our next step is to transform these arrays into a dataframe and then into a SparkSQL table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zippedValues: Array[(Double, Long)] = Array((47.794,24), (54.914,18), (62.034,32), (69.154,69), (76.274,54))\n",
       "defined class HistRow\n",
       "rowRDD: Array[HistRow] = Array(HistRow(47.794,24), HistRow(54.914,18), HistRow(62.034,32), HistRow(69.154,69), HistRow(76.274,54))\n",
       "histDF: org.apache.spark.sql.DataFrame = [startPoint: double, count: bigint]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val zippedValues = startValues.zip(counts)\n",
    "case class HistRow(startPoint:Double, count:Long)\n",
    "val rowRDD = zippedValues.map(value => HistRow(value._1, value._2))\n",
    "val histDF = spark.createDataFrame(rowRDD)\n",
    "histDF.createOrReplaceTempView(\"histogramTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|startPoint|count|\n",
      "+----------+-----+\n",
      "|    47.794|   24|\n",
      "|    54.914|   18|\n",
      "|    62.034|   32|\n",
      "|    69.154|   69|\n",
      "|    76.274|   54|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "histDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Apache Zeppelin plotting the histogram is a straightforward task once we have a SparkSQL table like **histogramTable**. Unfortunately this is not as simple in Jupyter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.- Outlier detection through the outlier labeling method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier labeling method is based on finding deviations with respect to the interquartile range (IQR):\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "where Q3 and Q1 are the 75th and 25th quantiles respectively. In this way, any value smaller than Q1- 1.5 * IQR or any value greater than Q3+1.5 * IQR will be categorised as an outlier.\n",
    "\n",
    "We will first create some dummy data to illustrate the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampleData: List[Double] = List(10.2, 14.1, 14.4, 14.4, 14.4, 14.5, 14.5, 14.6, 14.7, 14.7, 14.7, 14.9, 15.1, 15.9, 16.4)\n",
       "rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[43] at makeRDD at <console>:30\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampleData = List(10.2, 14.1, 14.4, 14.4, 14.4, 14.5, 14.5, 14.6, 14.7, 14.7, 14.7, 14.9, 15.1, 15.9, 16.4)\n",
    "   \n",
    "val rowRDD = spark.sparkContext.makeRDD(sampleData.map(value => Row(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(value,DoubleType,true))\n",
       "df: org.apache.spark.sql.DataFrame = [value: double]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = StructType(Array(StructField(\"value\", DoubleType)))\n",
    "\n",
    "val df = spark.createDataFrame(rowRDD, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the required quantiles and the IQR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quantiles: Array[Double] = Array(14.4, 14.9)\n",
       "Q1: Double = 14.4\n",
       "Q3: Double = 14.9\n",
       "IQR: Double = 0.5\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val quantiles = df.stat.approxQuantile(\"value\", Array(0.25,0.75),0.0)\n",
    "val Q1 = quantiles(0)\n",
    "val Q3 = quantiles(1)\n",
    "val IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can now filter the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "| 10.2|\n",
      "| 15.9|\n",
      "| 16.4|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lowerRange: Double = 13.65\n",
       "upperRange: Double = 15.65\n",
       "outliers: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [value: double]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lowerRange = Q1 - 1.5*IQR\n",
    "val upperRange = Q3 +  1.5*IQR\n",
    "\n",
    "val outliers = df.filter(s\"value < $lowerRange or value > $upperRange\")\n",
    "outliers.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
