{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A random forest classification engine with Spark ML\n",
    "\n",
    "Dr Jose M. Albornoz, May 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will build a classifier using Spark's sample classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-FQ2BOOJ:4040\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1558006945508)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
       "import org.apache.spark.ml.classification.RandomForestClassifier\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator}\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator}\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator  \n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator  \n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics  \n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics  \n",
    "import org.apache.spark.ml.classification.RandomForestClassifier  \n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator}  \n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator}  \n",
    "import org.apache.spark.ml.linalg.Vectors  \n",
    "import org.apache.spark.ml.Pipeline  \n",
    "import org.apache.log4j._  \n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Prepare training and test data.\n",
    "val data = spark.read.format(\"libsvm\").load(\"/home/jmalbornoz/Downloads/spark-2.4.0-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Long = 100\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Splitting the data by create an array of the training and test data\n",
    "val Array(training, test) = data.select(\"label\",\"features\").randomSplit(Array(0.7, 0.3), seed = 801)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Long = 71\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 29\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now create a model object (I’m using a Random Forest Classifier), define a parameter grid (I kept it simple and only varied the number of trees), create a Cross Validator object (here is where we set our scoring metric for training the model) and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_162afc2d4152\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create the model\n",
    "val rf = new RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_162afc2d4152-numTrees: 20\n",
       "}, {\n",
       "\trfc_162afc2d4152-numTrees: 50\n",
       "}, {\n",
       "\trfc_162afc2d4152-numTrees: 100\n",
       "})\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create the param grid\n",
    "val paramGrid = new ParamGridBuilder().addGrid(rf.numTrees,Array(20,50,100)).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_76a84baed0a1\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create cross val object, define scoring metric\n",
    "val cv = new CrossValidator().\n",
    "  setEstimator(rf).\n",
    "  setEvaluator(new BinaryClassificationEvaluator().setMetricName(\"areaUnderPR\")).\n",
    "  setEstimatorParamMaps(paramGrid).\n",
    "  setNumFolds(3).\n",
    "setParallelism(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.tuning.CrossValidatorModel = cv_76a84baed0a1\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// You can then treat this object as the model and use fit on it.\n",
    "val model = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[Double] = Array(1.0, 1.0, 1.0)\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.ml.Model[_] = RandomForestClassificationModel (uid=rfc_162afc2d4152) with 20 trees\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.- Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little more difficult because the evaluation functionality still mostly resides in the RDD-API for Spark, requiring some different syntax. Let’s begin by getting predictions on our test data and storing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = model.transform(test).select(\"features\", \"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(692,[121,122,123...|  0.0|       0.0|\n",
      "|(692,[122,123,124...|  0.0|       0.0|\n",
      "|(692,[123,124,125...|  0.0|       0.0|\n",
      "|(692,[123,124,125...|  0.0|       0.0|\n",
      "|(692,[124,125,126...|  0.0|       0.0|\n",
      "|(692,[124,125,126...|  0.0|       0.0|\n",
      "|(692,[124,125,126...|  0.0|       0.0|\n",
      "|(692,[126,127,128...|  0.0|       0.0|\n",
      "|(692,[127,128,129...|  0.0|       0.0|\n",
      "|(692,[153,154,155...|  0.0|       0.0|\n",
      "|(692,[154,155,156...|  0.0|       0.0|\n",
      "|(692,[123,124,125...|  1.0|       1.0|\n",
      "|(692,[123,124,125...|  1.0|       1.0|\n",
      "|(692,[123,124,125...|  1.0|       1.0|\n",
      "|(692,[123,124,125...|  1.0|       1.0|\n",
      "|(692,[124,125,126...|  1.0|       1.0|\n",
      "|(692,[125,126,127...|  1.0|       1.0|\n",
      "|(692,[125,126,127...|  1.0|       1.0|\n",
      "|(692,[125,126,153...|  1.0|       1.0|\n",
      "|(692,[126,127,128...|  1.0|       1.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then convert these results to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[604] at rdd at <console>:38\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionAndLabels = results.select($\"prediction\",$\"label\").as[(Double, Double)].rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create our metrics objects and print out the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "11.0  0.0   \n",
      "0.0   18.0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@46fad8a9\n",
       "mMetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@7908ae02\n",
       "labels: Array[Double] = Array(0.0, 1.0)\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Instantiate a new metrics objects\n",
    "val bMetrics = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val mMetrics = new MulticlassMetrics(predictionAndLabels)\n",
    "val labels = mMetrics.labels\n",
    "\n",
    "// Print out the Confusion matrix\n",
    "println(\"Confusion matrix:\")\n",
    "println(mMetrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the numbers in the confusion matrix to calculate some useful metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision(0.0) = 1.0\n",
      "Precision(1.0) = 1.0\n",
      "Recall(0.0) = 1.0\n",
      "Recall(1.0) = 1.0\n",
      "FPR(0.0) = 0.0\n",
      "FPR(1.0) = 0.0\n",
      "F1-Score(0.0) = 1.0\n",
      "F1-Score(1.0) = 1.0\n"
     ]
    }
   ],
   "source": [
    "// Precision by label\n",
    "labels.foreach { l =>\n",
    "  println(s\"Precision($l) = \" + mMetrics.precision(l))\n",
    "}\n",
    "\n",
    "// Recall by label\n",
    "labels.foreach { l =>\n",
    "  println(s\"Recall($l) = \" + mMetrics.recall(l))\n",
    "}\n",
    "\n",
    "// False positive rate by label\n",
    "labels.foreach { l =>\n",
    "  println(s\"FPR($l) = \" + mMetrics.falsePositiveRate(l))\n",
    "}\n",
    "\n",
    "// F-measure by label\n",
    "labels.foreach { l =>\n",
    "  println(s\"F1-Score($l) = \" + mMetrics.fMeasure(l))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 1.0, Precision: 1.0\n",
      "Threshold: 0.0, Precision: 0.6206896551724138\n",
      "Threshold: 1.0, Recall: 1.0\n",
      "Threshold: 0.0, Recall: 1.0\n",
      "Threshold: 1.0, F-score: 1.0, Beta = 1\n",
      "Threshold: 0.0, F-score: 0.7659574468085107, Beta = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "precision: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[621] at map at BinaryClassificationMetrics.scala:214\n",
       "recall: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[622] at map at BinaryClassificationMetrics.scala:214\n",
       "PRC: org.apache.spark.rdd.RDD[(Double, Double)] = UnionRDD[625] at union at BinaryClassificationMetrics.scala:110\n",
       "f1Score: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[626] at map at BinaryClassificationMetrics.scala:214\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Precision by threshold\n",
    "val precision = bMetrics.precisionByThreshold\n",
    "precision.foreach { case (t, p) =>\n",
    "  println(s\"Threshold: $t, Precision: $p\")\n",
    "}\n",
    "\n",
    "// Recall by threshold\n",
    "val recall = bMetrics.recallByThreshold\n",
    "recall.foreach { case (t, r) =>\n",
    "  println(s\"Threshold: $t, Recall: $r\")\n",
    "}\n",
    "\n",
    "// Precision-Recall Curve\n",
    "val PRC = bMetrics.pr\n",
    "\n",
    "// F-measure\n",
    "val f1Score = bMetrics.fMeasureByThreshold\n",
    "f1Score.foreach { case (t, f) =>\n",
    "  println(s\"Threshold: $t, F-score: $f, Beta = 1\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 1.0, F-score: 1.0, Beta = 0.5\n",
      "Threshold: 0.0, F-score: 0.7659574468085107, Beta = 0.5\n",
      "Area under precision-recall curve = 1.0\n",
      "Area under ROC = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "beta: Double = 0.5\n",
       "fScore: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[627] at map at BinaryClassificationMetrics.scala:214\n",
       "auPRC: Double = 1.0\n",
       "thresholds: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[633] at map at <console>:52\n",
       "roc: org.apache.spark.rdd.RDD[(Double, Double)] = UnionRDD[637] at UnionRDD at BinaryClassificationMetrics.scala:90\n",
       "auROC: Double = 1.0\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val beta = 0.5\n",
    "val fScore = bMetrics.fMeasureByThreshold(beta)\n",
    "f1Score.foreach { case (t, f) =>\n",
    "  println(s\"Threshold: $t, F-score: $f, Beta = 0.5\")\n",
    "}\n",
    "\n",
    "// AUPRC\n",
    "val auPRC = bMetrics.areaUnderPR\n",
    "println(\"Area under precision-recall curve = \" + auPRC)\n",
    "\n",
    "// Compute thresholds used in ROC and PR curves\n",
    "val thresholds = precision.map(_._1)\n",
    "\n",
    "// ROC Curve\n",
    "val roc = bMetrics.roc\n",
    "\n",
    "// AUROC\n",
    "val auROC = bMetrics.areaUnderROC\n",
    "println(\"Area under ROC = \" + auROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
