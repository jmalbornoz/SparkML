{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A random forest classification engine with Spark ML\n",
    "\n",
    "Dr Jose M. Albornoz, April 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will build a classifier that predicts whether a student passes a course based on data accumulated throughout the entire course. I will use he Harvard EdX to be found at https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/26147"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’m not going to do much feature engineering because I want to focus on the mechanics of training the model in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.192:4040\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1556116701375)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
       "import org.apache.spark.ml.classification.RandomForestClassifier\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator}\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator}\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator  \n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics  \n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics  \n",
    "import org.apache.spark.ml.classification.RandomForestClassifier  \n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator}  \n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator}  \n",
    "import org.apache.spark.ml.linalg.Vectors  \n",
    "import org.apache.spark.ml.Pipeline  \n",
    "import org.apache.log4j._  \n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [course_id: string, userid_DI: string ... 18 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").format(\"csv\").load(\"mooc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+----------+------+--------+---------+-----------------+------+---+------+-----+-------------------+-------------------+-------+---------+-----------+---------+------------+-----+---------------+\n",
      "|           course_id|     userid_DI|registered|viewed|explored|certified|final_cc_cname_DI|LoE_DI|YoB|gender|grade|      start_time_DI|      last_event_DI|nevents|ndays_act|nplay_video|nchapters|nforum_posts|roles|incomplete_flag|\n",
      "+--------------------+--------------+----------+------+--------+---------+-----------------+------+---+------+-----+-------------------+-------------------+-------+---------+-----------+---------+------------+-----+---------------+\n",
      "|HarvardX/CB22x/20...|MHxPC130442623|         1|     0|       0|        0|    United States|    NA| NA|    NA|    0|2012-12-19 00:00:00|2013-11-17 00:00:00|   null|        9|       null|     null|           0| null|              1|\n",
      "| HarvardX/CS50x/2012|MHxPC130442623|         1|     1|       0|        0|    United States|    NA| NA|    NA|    0|2012-10-15 00:00:00|               null|   null|        9|       null|      1.0|           0| null|              1|\n",
      "|HarvardX/CB22x/20...|MHxPC130275857|         1|     0|       0|        0|    United States|    NA| NA|    NA|    0|2013-02-08 00:00:00|2013-11-17 00:00:00|   null|       16|       null|     null|           0| null|              1|\n",
      "| HarvardX/CS50x/2012|MHxPC130275857|         1|     0|       0|        0|    United States|    NA| NA|    NA|    0|2012-09-17 00:00:00|               null|   null|       16|       null|     null|           0| null|              1|\n",
      "|HarvardX/ER22x/20...|MHxPC130275857|         1|     0|       0|        0|    United States|    NA| NA|    NA|    0|2012-12-19 00:00:00|               null|   null|       16|       null|     null|           0| null|              1|\n",
      "+--------------------+--------------+----------+------+--------+---------+-----------------+------+---+------+-----+-------------------+-------------------+-------+---------+-----------+---------+------------+-----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.- Selection of relevant columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Spark model needs exactly two columns: “label” and “features”. To get there will take a few steps. First we will identify our label using the select method while also keeping only relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [label: int, registered: int ... 9 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = (data.select(data(\"certified\").as(\"label\"), $\"registered\", $\"viewed\", $\"explored\", \n",
    "          $\"final_cc_cname_DI\", $\"gender\", $\"nevents\", $\"ndays_act\", $\"nplay_video\", $\"nchapters\", $\"nforum_posts\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the entire method call in a set of parentheses allows you to break up the lines arbitrarily without Spark freaking out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+--------+-----------------+------+-------+---------+-----------+---------+------------+\n",
      "|label|registered|viewed|explored|final_cc_cname_DI|gender|nevents|ndays_act|nplay_video|nchapters|nforum_posts|\n",
      "+-----+----------+------+--------+-----------------+------+-------+---------+-----------+---------+------------+\n",
      "|    0|         1|     0|       0|    United States|    NA|   null|        9|       null|     null|           0|\n",
      "|    0|         1|     1|       0|    United States|    NA|   null|        9|       null|      1.0|           0|\n",
      "|    0|         1|     0|       0|    United States|    NA|   null|       16|       null|     null|           0|\n",
      "|    0|         1|     0|       0|    United States|    NA|   null|       16|       null|     null|           0|\n",
      "|    0|         1|     0|       0|    United States|    NA|   null|       16|       null|     null|           0|\n",
      "+-----+----------+------+--------+-----------------+------+-------+---------+-----------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Long = 641138\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.- One-hot encoding of categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will do some one-hot encoding on our categorical features. This takes a few steps. First we have to use the StringIndexer to convert the strings to integers. Then we have to use the OneHotEncoderEstimator to do the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexer1: org.apache.spark.ml.feature.StringIndexer = strIdx_025904f265de\n",
       "indexed1: org.apache.spark.sql.DataFrame = [label: int, registered: int ... 10 more fields]\n",
       "indexer2: org.apache.spark.ml.feature.StringIndexer = strIdx_446e86b7c0d5\n",
       "indexed2: org.apache.spark.sql.DataFrame = [label: int, registered: int ... 11 more fields]\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_56d9833e7f4e\n",
       "encoded: org.apache.spark.sql.DataFrame = [label: int, registered: int ... 13 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// string indexing\n",
    "val indexer1 = new StringIndexer().\n",
    "    setInputCol(\"final_cc_cname_DI\").\n",
    "    setOutputCol(\"countryIndex\").\n",
    "    setHandleInvalid(\"keep\") \n",
    "val indexed1 = indexer1.fit(df).transform(df)\n",
    "\n",
    "val indexer2 = new StringIndexer().\n",
    "    setInputCol(\"gender\").\n",
    "    setOutputCol(\"genderIndex\").\n",
    "    setHandleInvalid(\"keep\")\n",
    "val indexed2 = indexer2.fit(indexed1).transform(indexed1)\n",
    "\n",
    "// one hot encoding\n",
    "val encoder = new OneHotEncoderEstimator().\n",
    "  setInputCols(Array(\"countryIndex\", \"genderIndex\")).\n",
    "  setOutputCols(Array(\"countryVec\", \"genderVec\"))\n",
    "val encoded = encoder.fit(indexed2).transform(indexed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the *.setHandleInvalid(\"keep\")* option the indexer adds new indexes whenever it sees new labels (which may happen on a test set, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+--------+-----------------+------+-------+---------+-----------+---------+------------+------------+-----------+--------------+-------------+\n",
      "|label|registered|viewed|explored|final_cc_cname_DI|gender|nevents|ndays_act|nplay_video|nchapters|nforum_posts|countryIndex|genderIndex|    countryVec|    genderVec|\n",
      "+-----+----------+------+--------+-----------------+------+-------+---------+-----------+---------+------------+------------+-----------+--------------+-------------+\n",
      "|    0|         1|     0|       0|    United States|    NA|   null|        9|       null|     null|           0|         0.0|        2.0|(34,[0],[1.0])|(4,[2],[1.0])|\n",
      "|    0|         1|     1|       0|    United States|    NA|   null|        9|       null|      1.0|           0|         0.0|        2.0|(34,[0],[1.0])|(4,[2],[1.0])|\n",
      "|    0|         1|     0|       0|    United States|    NA|   null|       16|       null|     null|           0|         0.0|        2.0|(34,[0],[1.0])|(4,[2],[1.0])|\n",
      "|    0|         1|     0|       0|    United States|    NA|   null|       16|       null|     null|           0|         0.0|        2.0|(34,[0],[1.0])|(4,[2],[1.0])|\n",
      "|    0|         1|     0|       0|    United States|    NA|   null|       16|       null|     null|           0|         0.0|        2.0|(34,[0],[1.0])|(4,[2],[1.0])|\n",
      "+-----+----------+------+--------+-----------------+------+-------+---------+-----------+---------+------------+------------+-----------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.- Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------------+\n",
      "|nevents| count|         proportion|\n",
      "+-------+------+-------------------+\n",
      "|   null|199151| 31.062111433108004|\n",
      "|      1| 63565|  9.914402203581757|\n",
      "|      2| 34329|  5.354385483312485|\n",
      "|      3| 17669| 2.7558809491872265|\n",
      "|      4| 12217|  1.905518000804819|\n",
      "|      5|  9850|  1.536330711952809|\n",
      "|      6|  8480| 1.3226481662294234|\n",
      "|      7|  7259| 1.1322055470117198|\n",
      "|      8|  6648|  1.036906251072312|\n",
      "|      9|  6076| 0.9476898889162707|\n",
      "|     10|  5621| 0.8767223281103288|\n",
      "|     11|  5197| 0.8105899197988576|\n",
      "|     12|  4870| 0.7595868596152466|\n",
      "|     13|  4476| 0.6981336311371343|\n",
      "|     14|  4222| 0.6585165752146964|\n",
      "|     15|  3891| 0.6068896243866375|\n",
      "|     16|  3808| 0.5939438935143448|\n",
      "|     17|  3545| 0.5529230836418992|\n",
      "|     18|  3349|  0.522352442063955|\n",
      "|     19|  3144|0.49037804653600314|\n",
      "+-------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nanEvents: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [nevents: int, count: bigint]\n",
       "nanEvents1: org.apache.spark.sql.DataFrame = [nevents: int, count: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nanEvents = encoded.groupBy(\"nevents\").count().orderBy($\"count\".desc)\n",
    "val nanEvents1 = nanEvents.withColumn(\"proportion\", $\"count\"*100/641138)\n",
    "nanEvents1.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------------------+\n",
      "|ndays_act| count|         proportion|\n",
      "+---------+------+-------------------+\n",
      "|        1|209941| 32.745056446506055|\n",
      "|     null|162743| 25.383458787343756|\n",
      "|        2| 80625| 12.575295802151798|\n",
      "|        3| 43081|  6.719458213364361|\n",
      "|        4| 26813| 4.1820949623949915|\n",
      "|        5| 18552| 2.8936048089490876|\n",
      "|        6| 13239| 2.0649220604612424|\n",
      "|        7| 10281| 1.6035549288920639|\n",
      "|        8|  8075| 1.2594792384790794|\n",
      "|        9|  6510| 1.0153820238388616|\n",
      "|       10|  5324| 0.8303984477600767|\n",
      "|       11|  4415| 0.6886192988093047|\n",
      "|       12|  3815| 0.5950357021421285|\n",
      "|       13|  3323| 0.5182971528750441|\n",
      "|       14|  2794| 0.4357876151468171|\n",
      "|       15|  2542|0.39648250454660305|\n",
      "|       16|  2146| 0.3347173307462668|\n",
      "|       17|  2002| 0.3122572675461445|\n",
      "|       18|  1852|0.28886136837935045|\n",
      "|       19|  1564| 0.2439412419791059|\n",
      "+---------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nanNdays: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ndays_act: int, count: bigint]\n",
       "nanNdays1: org.apache.spark.sql.DataFrame = [ndays_act: int, count: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nanNdays = encoded.groupBy(\"ndays_act\").count().orderBy($\"count\".desc)\n",
    "val nanNdays1 = nanNdays.withColumn(\"proportion\", $\"count\"*100/641138)\n",
    "nanNdays1.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-------------------+\n",
      "|nplay_video| count|         proportion|\n",
      "+-----------+------+-------------------+\n",
      "|       null|457530|   71.3621716385552|\n",
      "|          1| 16968| 2.6465441137477423|\n",
      "|          2| 11000| 1.7156992722315632|\n",
      "|          3|  8371| 1.3056471461682195|\n",
      "|          4|  6995| 1.0910287644781622|\n",
      "|          5|  5992| 0.9345881853828661|\n",
      "|          6|  5373| 0.8380411081545627|\n",
      "|          7|  4714| 0.7352551244817809|\n",
      "|          8|  4296| 0.6700585521369814|\n",
      "|          9|  4076| 0.6357445666923501|\n",
      "|         10|  3620| 0.5646210332252962|\n",
      "|         11|  3453| 0.5385735988195989|\n",
      "|         12|  3187| 0.4970848709638175|\n",
      "|         13|  2853|0.44499000215242274|\n",
      "|         14|  2641|0.41192379799668716|\n",
      "|         15|  2453| 0.3826009377076386|\n",
      "|         16|  2401|0.37449035932981667|\n",
      "|         17|  2138| 0.3334695494573711|\n",
      "|         18|  2085| 0.3252029984184372|\n",
      "|         19|  1887| 0.2943204115182691|\n",
      "+-----------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nanPlayVideo: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [nplay_video: int, count: bigint]\n",
       "nanPlayVideo1: org.apache.spark.sql.DataFrame = [nplay_video: int, count: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nanPlayVideo = encoded.groupBy(\"nplay_video\").count().orderBy($\"count\".desc)\n",
    "val nanPlayVideo1 = nanPlayVideo.withColumn(\"proportion\", $\"count\"*100/641138)\n",
    "nanPlayVideo1.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------------------+\n",
      "|nchapters| count|         proportion|\n",
      "+---------+------+-------------------+\n",
      "|     null|258753|  40.35839398070306|\n",
      "|      1.0|121837| 19.003241111897907|\n",
      "|      2.0|110085|  17.17025039851015|\n",
      "|      3.0| 52296|  8.156746285511076|\n",
      "|      4.0| 24937|  3.889490250148954|\n",
      "|      5.0| 13838| 2.1583496844673067|\n",
      "|      6.0|  8536|  1.331382635251693|\n",
      "|     12.0|  7987|  1.245753644301227|\n",
      "|      7.0|  6556| 1.0225567662500117|\n",
      "|      8.0|  5009| 0.7812670595098091|\n",
      "|      9.0|  4091| 0.6380841566090296|\n",
      "|     10.0|  3598| 0.5611896346808332|\n",
      "|     18.0|  3411| 0.5320227470528965|\n",
      "|     11.0|  3258| 0.5081589299027667|\n",
      "|     16.0|  2890| 0.4507609906135652|\n",
      "|     15.0|  2684|0.41863062242450144|\n",
      "|     13.0|  2053| 0.3202118732628545|\n",
      "|     14.0|  2021|0.31522074810727174|\n",
      "|     17.0|  1877|0.29276068490714946|\n",
      "|     32.0|   657|0.10247403835055792|\n",
      "+---------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nanNChapters: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [nchapters: double, count: bigint]\n",
       "nanNChapters1: org.apache.spark.sql.DataFrame = [nchapters: double, count: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nanNChapters = encoded.groupBy(\"nchapters\").count().orderBy($\"count\".desc)\n",
    "val nanNChapters1 = nanNChapters.withColumn(\"proportion\", $\"count\"*100/641138)\n",
    "nanNChapters1.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some columns the proportion of null values reaches 71% - we will impute the null values of the above explored columns using the median value of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neventsMedianArray: Array[Double] = Array(24.0)\n",
       "neventsMedian: Double = 24.0\n",
       "ndays_actMedianArray: Array[Double] = Array(2.0)\n",
       "ndays_actMedian: Double = 2.0\n",
       "nplay_videoMedianArray: Array[Double] = Array(18.0)\n",
       "nplay_videoMedian: Double = 18.0\n",
       "nchaptersMedianArray: Array[Double] = Array(2.0)\n",
       "nchaptersMedian: Double = 2.0\n",
       "filled: org.apache.spark.sql.DataFrame = [label: int, registered: int ... 13 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define medians\n",
    "val neventsMedianArray = encoded.stat.approxQuantile(\"nevents\", Array(0.5), 0)\n",
    "val neventsMedian = neventsMedianArray(0)\n",
    "\n",
    "val ndays_actMedianArray = encoded.stat.approxQuantile(\"ndays_act\", Array(0.5), 0)\n",
    "val ndays_actMedian = ndays_actMedianArray(0)\n",
    "\n",
    "val nplay_videoMedianArray = encoded.stat.approxQuantile(\"nplay_video\", Array(0.5), 0)\n",
    "val nplay_videoMedian = nplay_videoMedianArray(0)\n",
    "\n",
    "val nchaptersMedianArray = encoded.stat.approxQuantile(\"nchapters\", Array(0.5), 0)\n",
    "val nchaptersMedian = nchaptersMedianArray(0)\n",
    "\n",
    "// replace \n",
    "val filled = encoded.na.fill(Map(\n",
    "  \"nevents\" -> neventsMedian, \n",
    "  \"ndays_act\" -> ndays_actMedian, \n",
    "  \"nplay_video\" -> nplay_videoMedian, \n",
    "\"nchapters\" -> nchaptersMedian))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.- Construction of features column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the VectorAssembler object to construct our features column. Remember, Spark models need exactly two columns: “label” and “features”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_aaa4867dfdb2\n",
       "output: org.apache.spark.sql.DataFrame = [label: int, features: vector]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Set the input columns as the features we want to use\n",
    "val assembler = (new VectorAssembler().setInputCols(Array(\n",
    "  \"viewed\", \"explored\", \"nevents\", \"ndays_act\", \"nplay_video\", \n",
    "  \"nchapters\", \"nforum_posts\", \"countryVec\", \"genderVec\")).\n",
    "   setOutputCol(\"features\"))\n",
    "\n",
    "// Transform the DataFrame\n",
    "val output = assembler.transform(filled).select($\"label\",$\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(45,[2,3,4,5,7,43...|\n",
      "|    0|(45,[0,2,3,4,5,7,...|\n",
      "|    0|(45,[2,3,4,5,7,43...|\n",
      "|    0|(45,[2,3,4,5,7,43...|\n",
      "|    0|(45,[2,3,4,5,7,43...|\n",
      "|    0|(45,[0,1,2,3,4,5,...|\n",
      "|    0|(45,[2,3,4,5,7,43...|\n",
      "|    0|(45,[0,2,3,4,5,32...|\n",
      "|    0|(45,[0,2,3,4,5,7,...|\n",
      "|    0|(45,[0,2,3,4,5,7,...|\n",
      "|    0|(45,[0,2,3,4,5,7,...|\n",
      "|    0|(45,[0,2,3,4,5,7,...|\n",
      "|    0|(45,[0,2,3,4,5,7,...|\n",
      "|    0|(45,[0,2,3,4,5,7,...|\n",
      "|    0|(45,[0,2,3,4,5,7,...|\n",
      "|    0|(45,[0,2,3,4,5,7,...|\n",
      "|    0|(45,[0,2,3,4,5,7,...|\n",
      "|    0|(45,[0,2,3,4,5,7,...|\n",
      "|    0|(45,[0,2,3,4,5,6,...|\n",
      "|    0|(45,[0,2,3,4,5,9,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.- Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, features: vector]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, features: vector]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Splitting the data by create an array of the training and test data\n",
    "val Array(training, test) = output.select(\"label\",\"features\").randomSplit(Array(0.7, 0.3), seed = 801)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Long = 449388\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Long = 191750\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now create a model object (I’m using a Random Forest Classifier), define a parameter grid (I kept it simple and only varied the number of trees), create a Cross Validator object (here is where we set our scoring metric for training the model) and fit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: This code will take some time to run! If you have a particularly old / underpowered computer, beware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_50f023f527ce\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create the model\n",
    "val rf = new RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_50f023f527ce-numTrees: 20\n",
       "}, {\n",
       "\trfc_50f023f527ce-numTrees: 50\n",
       "}, {\n",
       "\trfc_50f023f527ce-numTrees: 100\n",
       "})\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create the param grid\n",
    "val paramGrid = new ParamGridBuilder().addGrid(rf.numTrees,Array(20,50,100)).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_17e12cb8b37e\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create cross val object, define scoring metric\n",
    "val cv = new CrossValidator().\n",
    "  setEstimator(rf).\n",
    "  setEvaluator(new MulticlassClassificationEvaluator().setMetricName(\"weightedRecall\")).\n",
    "  setEstimatorParamMaps(paramGrid).\n",
    "  setNumFolds(3).\n",
    "setParallelism(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1- Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.tuning.CrossValidatorModel = cv_17e12cb8b37e\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// You can then treat this object as the model and use fit on it.\n",
    "val model = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Array[Double] = Array(0.9846214019525908, 0.9847726928730518, 0.9847926896199338)\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: org.apache.spark.ml.Model[_] = RandomForestClassificationModel (uid=rfc_50f023f527ce) with 100 trees\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2- Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little more difficult because the evaluation functionality still mostly resides in the RDD-API for Spark, requiring some different syntax. Let’s begin by getting predictions on our test data and storing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [features: vector, label: int ... 1 more field]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = model.transform(test).select(\"features\", \"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       1.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       1.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       1.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       1.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       1.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "|(45,[0,1,2,3,4,5,...|    0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then convert these results to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[719] at rdd at <console>:37\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionAndLabels = results.select($\"prediction\",$\"label\").as[(Double, Double)].rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create our metrics objects and print out the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "185595.0  908.0   \n",
      "1960.0    3287.0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@7a4a9825\n",
       "mMetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@7deee5ac\n",
       "labels: Array[Double] = Array(0.0, 1.0)\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Instantiate a new metrics objects\n",
    "val bMetrics = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val mMetrics = new MulticlassMetrics(predictionAndLabels)\n",
    "val labels = mMetrics.labels\n",
    "\n",
    "// Print out the Confusion matrix\n",
    "println(\"Confusion matrix:\")\n",
    "println(mMetrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the numbers in the confusion matrix to calculate some useful metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision(0.0) = 0.9895497320785903\n",
      "Precision(1.0) = 0.7835518474374255\n",
      "Recall(0.0) = 0.9951314456067731\n",
      "Recall(1.0) = 0.6264532113588718\n",
      "FPR(0.0) = 0.3735467886411283\n",
      "FPR(1.0) = 0.004868554393226919\n",
      "F1-Score(0.0) = 0.9923327398424844\n",
      "F1-Score(1.0) = 0.6962507943232367\n"
     ]
    }
   ],
   "source": [
    "// Precision by label\n",
    "labels.foreach { l =>\n",
    "  println(s\"Precision($l) = \" + mMetrics.precision(l))\n",
    "}\n",
    "\n",
    "// Recall by label\n",
    "labels.foreach { l =>\n",
    "  println(s\"Recall($l) = \" + mMetrics.recall(l))\n",
    "}\n",
    "\n",
    "// False positive rate by label\n",
    "labels.foreach { l =>\n",
    "  println(s\"FPR($l) = \" + mMetrics.falsePositiveRate(l))\n",
    "}\n",
    "\n",
    "// F-measure by label\n",
    "labels.foreach { l =>\n",
    "  println(s\"F1-Score($l) = \" + mMetrics.fMeasure(l))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 1.0, Precision: 0.7835518474374255\n",
      "Threshold: 0.0, Precision: 0.027363754889178617\n",
      "Threshold: 0.0, Recall: 1.0\n",
      "Threshold: 1.0, Recall: 0.6264532113588718\n",
      "Threshold: 0.0, F-score: 0.053269846748935264, Beta = 1\n",
      "Threshold: 1.0, F-score: 0.6962507943232367, Beta = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "precision: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[738] at map at BinaryClassificationMetrics.scala:214\n",
       "recall: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[739] at map at BinaryClassificationMetrics.scala:214\n",
       "PRC: org.apache.spark.rdd.RDD[(Double, Double)] = UnionRDD[742] at union at BinaryClassificationMetrics.scala:110\n",
       "f1Score: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[743] at map at BinaryClassificationMetrics.scala:214\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Precision by threshold\n",
    "val precision = bMetrics.precisionByThreshold\n",
    "precision.foreach { case (t, p) =>\n",
    "  println(s\"Threshold: $t, Precision: $p\")\n",
    "}\n",
    "\n",
    "// Recall by threshold\n",
    "val recall = bMetrics.recallByThreshold\n",
    "recall.foreach { case (t, r) =>\n",
    "  println(s\"Threshold: $t, Recall: $r\")\n",
    "}\n",
    "\n",
    "// Precision-Recall Curve\n",
    "val PRC = bMetrics.pr\n",
    "\n",
    "// F-measure\n",
    "val f1Score = bMetrics.fMeasureByThreshold\n",
    "f1Score.foreach { case (t, f) =>\n",
    "  println(s\"Threshold: $t, F-score: $f, Beta = 1\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 1.0, F-score: 0.6962507943232367, Beta = 0.5\n",
      "Threshold: 0.0, F-score: 0.053269846748935264, Beta = 0.5\n",
      "Area under precision-recall curve = 0.6423160306473965\n",
      "Area under ROC = 0.8107923284828225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "beta: Double = 0.5\n",
       "fScore: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[744] at map at BinaryClassificationMetrics.scala:214\n",
       "auPRC: Double = 0.6423160306473965\n",
       "thresholds: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[750] at map at <console>:51\n",
       "roc: org.apache.spark.rdd.RDD[(Double, Double)] = UnionRDD[754] at UnionRDD at BinaryClassificationMetrics.scala:90\n",
       "auROC: Double = 0.8107923284828225\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val beta = 0.5\n",
    "val fScore = bMetrics.fMeasureByThreshold(beta)\n",
    "f1Score.foreach { case (t, f) =>\n",
    "  println(s\"Threshold: $t, F-score: $f, Beta = 0.5\")\n",
    "}\n",
    "\n",
    "// AUPRC\n",
    "val auPRC = bMetrics.areaUnderPR\n",
    "println(\"Area under precision-recall curve = \" + auPRC)\n",
    "\n",
    "// Compute thresholds used in ROC and PR curves\n",
    "val thresholds = precision.map(_._1)\n",
    "\n",
    "// ROC Curve\n",
    "val roc = bMetrics.roc\n",
    "\n",
    "// AUROC\n",
    "val auROC = bMetrics.areaUnderROC\n",
    "println(\"Area under ROC = \" + auROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
