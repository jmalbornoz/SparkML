{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified train-test split in Spark\n",
    "\n",
    "Dr Jose M Albornoz, June 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a stratified train-test split by following these steps:\n",
    "\n",
    "* Determine how many examples of every label should be a part of train set given some ratio.\n",
    "* Shuffle the rows of the DataFrame.\n",
    "* Use window function to partition and order the DataFrame by label and then rank each label's observations using row_number()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-FQ2BOOJ:4040\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1560170334163)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.{DataFrame, Row}\n",
       "import org.apache.spark.sql.functions.{col, row_number, udf, rand}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.{DataFrame, Row}\n",
    "import org.apache.spark.sql.functions.{col, row_number, udf, rand}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Some test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: Seq[(Int, Double)] = List((0,0.0), (1,1.0), (2,0.0), (3,1.0), (4,0.0), (5,1.0), (6,0.0), (7,1.0), (8,0.0), (9,1.0))\n",
       "df0: org.apache.spark.sql.DataFrame = [id: int, label: double]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq((0, 0.0), (1, 1.0), (2, 0.0), (3, 1.0), (4, 0.0), (5, 1.0), (6, 0.0), (7, 1.0), (8, 0.0), (9, 1.0)) \n",
    "val df0 = data.toDF(\"id\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|label|\n",
      "+---+-----+\n",
      "|  0|  0.0|\n",
      "|  1|  1.0|\n",
      "|  2|  0.0|\n",
      "|  3|  1.0|\n",
      "|  4|  0.0|\n",
      "|  5|  1.0|\n",
      "|  6|  0.0|\n",
      "|  7|  1.0|\n",
      "|  8|  0.0|\n",
      "|  9|  1.0|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getNumExamplesPerClass: (df: org.apache.spark.sql.DataFrame, trainRatio: Double)Map[Int,Long]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getNumExamplesPerClass(df: DataFrame, trainRatio: Double): Map[Int, Long] = {\n",
    "    \n",
    "    val countZeroes = (df.filter(df(\"label\") === 0).count*trainRatio).toLong\n",
    "    val countOnes = (df.filter(df(\"label\") === 1).count*trainRatio).toLong\n",
    "    Map(0 -> countZeroes, 1 -> countOnes)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Map[Int,Long] = Map(0 -> 4, 1 -> 4)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getNumExamplesPerClass(df0, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stratifiedTrainTestSplit: (df: org.apache.spark.sql.DataFrame, label: String, trainRatio: Double)Array[org.apache.spark.sql.DataFrame]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stratifiedTrainTestSplit(df: DataFrame, label: String, trainRatio: Double) = {\n",
    "        \n",
    "    val w = Window.partitionBy(col(label)).orderBy(col(label))\n",
    "\n",
    "    val rowNumPartitioner = row_number().over(w)\n",
    "\n",
    "    val dfRowNum = df.sort(rand).select(col(\"*\"), rowNumPartitioner as \"row_number\")\n",
    "\n",
    "    val observationsPerLabel: Map[Int, Long] = getNumExamplesPerClass(df, trainRatio)\n",
    "\n",
    "    val addIsTrainColumn = udf((label: Int, rowNumber: Int) => rowNumber <= observationsPerLabel(label))\n",
    "\n",
    "    val df_mark = dfRowNum.withColumn(\"isTrainSet\", addIsTrainColumn(col(label), col(\"row_number\")))\n",
    "    \n",
    "    val colsToRemove = Seq(\"row_number\", \"isTrainSet\") \n",
    "    \n",
    "    Array(df_mark.where(col(\"isTrainSet\") === true).drop(colsToRemove : _*), \n",
    "          df_mark.where(col(\"isTrainSet\") === false).drop(colsToRemove : _*))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.DataFrame = [id: int, label: double]\n",
       "test: org.apache.spark.sql.DataFrame = [id: int, label: double]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train, test) = stratifiedTrainTestSplit(df0, \"label\", 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|label|\n",
      "+---+-----+\n",
      "|  8|  0.0|\n",
      "|  6|  0.0|\n",
      "|  4|  0.0|\n",
      "|  2|  0.0|\n",
      "|  1|  1.0|\n",
      "|  9|  1.0|\n",
      "|  7|  1.0|\n",
      "|  5|  1.0|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|label|\n",
      "+---+-----+\n",
      "|  0|  0.0|\n",
      "|  3|  1.0|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
