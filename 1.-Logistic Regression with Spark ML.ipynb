{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Spark ML\n",
    "\n",
    "Dr Jose M. Albornoz, January 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-09 16:16:03 WARN  LibSVMFileFormat:66 - 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training = spark.read.format(\"libsvm\").load(\"/home/jmalbornoz/Downloads/spark-2.4.0-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Invoke logistic regression object. elasticNetParam corresponds to α and regParam corresponds to λ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_84e198909871\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.3)\n",
    "  .setElasticNetParam(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_84e198909871, numClasses = 2, numFeatures = 692\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = lr.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.- Print coefficient and intercept for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (692,[244,263,272,300,301,328,350,351,378,379,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.353983524188197E-5,-9.102738505589466E-5,-1.9467430546904298E-4,-2.0300642473486668E-4,-3.1476183314863995E-5,-6.842977602660743E-5,1.5883626898239883E-5,1.4023497091372047E-5,3.5432047524968605E-4,1.1443272898171087E-4,1.0016712383666666E-4,6.014109303795481E-4,2.840248179122762E-4,-1.1541084736508837E-4,3.85996886312906E-4,6.35019557424107E-4,-1.1506412384575676E-4,-1.5271865864986808E-4,2.804933808994214E-4,6.070117471191634E-4,-2.008459663247437E-4,-1.421075579290126E-4,2.739010341160883E-4,2.7730456244968115E-4,-9.838027027269332E-5,-3.808522443517704E-4,-2.5315198008555033E-4,2.7747714770754307E-4,-2.443619763919199E-4,-0.0015394744687597765,-2.3073328411331293E-4]) Intercept: 0.22456315961250325\n"
     ]
    }
   ],
   "source": [
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.- We can also use the multinomial family for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlr: org.apache.spark.ml.classification.LogisticRegression = logreg_fedc48108156\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlr = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.3)\n",
    "  .setElasticNetParam(0.8)\n",
    "  .setFamily(\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_fedc48108156, numClasses = 2, numFeatures = 692\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlrModel = mlr.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.- Print the coefficients and intercepts for logistic regression with multinomial family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial coefficients: 2 x 692 CSCMatrix\n",
      "(0,244) 4.290365458958277E-5\n",
      "(1,244) -4.290365458958294E-5\n",
      "(0,263) 6.488313287833108E-5\n",
      "(1,263) -6.488313287833092E-5\n",
      "(0,272) 1.2140666790834663E-4\n",
      "(1,272) -1.2140666790834657E-4\n",
      "(0,300) 1.3231861518665612E-4\n",
      "(1,300) -1.3231861518665607E-4\n",
      "(0,350) -6.775444746760509E-7\n",
      "(1,350) 6.775444746761932E-7\n",
      "(0,351) -4.899237909429297E-7\n",
      "(1,351) 4.899237909430322E-7\n",
      "(0,378) -3.5812102770679596E-5\n",
      "(1,378) 3.581210277067968E-5\n",
      "(0,379) -2.3539704331222065E-5\n",
      "(1,379) 2.353970433122204E-5\n",
      "(0,405) -1.90295199030314E-5\n",
      "(1,405) 1.90295199030314E-5\n",
      "(0,406) -5.626696935778909E-4\n",
      "(1,406) 5.626696935778912E-4\n",
      "(0,407) -5.121519619099504E-5\n",
      "(1,407) 5.1215196190995074E-5\n",
      "(0,428) 8.080614545413342E-5\n",
      "(1,428) -8.080614545413331E-5\n",
      "(0,433) -4.256734915330487E-5\n",
      "(1,433) 4.256734915330495E-5\n",
      "(0,434) -7.080191510151425E-4\n",
      "(1,434) 7.080191510151435E-4\n",
      "(0,455) 8.094482475733589E-5\n",
      "(1,455) -8.094482475733582E-5\n",
      "(0,456) 1.0433687128309833E-4\n",
      "(1,456) -1.0433687128309814E-4\n",
      "(0,461) -5.4466605046259246E-5\n",
      "(1,461) 5.4466605046259286E-5\n",
      "(0,462) -5.667133061990392E-4\n",
      "(1,462) 5.667133061990392E-4\n",
      "(0,483) 1.2495896045528374E-4\n",
      "(1,483) -1.249589604552838E-4\n",
      "(0,484) 9.810519424784944E-5\n",
      "(1,484) -9.810519424784941E-5\n",
      "(0,489) -4.88440907254626E-5\n",
      "(1,489) 4.8844090725462606E-5\n",
      "(0,490) -4.324392733454803E-5\n",
      "(1,490) 4.324392733454811E-5\n",
      "(0,496) 6.903351855620161E-5\n",
      "(1,496) -6.90335185562012E-5\n",
      "(0,511) 3.946505594172827E-4\n",
      "(1,511) -3.946505594172831E-4\n",
      "(0,512) 2.621745995919226E-4\n",
      "(1,512) -2.621745995919226E-4\n",
      "(0,517) -4.459475951170906E-5\n",
      "(1,517) 4.459475951170901E-5\n",
      "(0,539) 2.5417562428184555E-4\n",
      "(1,539) -2.5417562428184555E-4\n",
      "(0,540) 5.271781246228031E-4\n",
      "(1,540) -5.271781246228032E-4\n",
      "(0,568) 1.860255150352447E-4\n",
      "(1,568) -1.8602551503524485E-4\n",
      "Multinomial intercepts: [-0.12065879445860686,0.12065879445860686]\n"
     ]
    }
   ],
   "source": [
    "println(s\"Multinomial coefficients: ${mlrModel.coefficientMatrix}\")\n",
    "println(s\"Multinomial intercepts: ${mlrModel.interceptVector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.-  Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingSummary: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@396171a8\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.- Obtain the objective per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.6833149135741672\n",
      "0.6662875751473734\n",
      "0.6217068546034618\n",
      "0.6127265245887887\n",
      "0.6060347986802873\n",
      "0.6031750687571562\n",
      "0.5969621534836274\n",
      "0.5940743031983118\n",
      "0.5906089243339022\n",
      "0.5894724576491042\n",
      "0.5882187775729587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "objectiveHistory: Array[Double] = Array(0.6833149135741672, 0.6662875751473734, 0.6217068546034618, 0.6127265245887887, 0.6060347986802873, 0.6031750687571562, 0.5969621534836274, 0.5940743031983118, 0.5906089243339022, 0.5894724576491042, 0.5882187775729587)\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val objectiveHistory = trainingSummary.objectiveHistory\n",
    "println(\"objectiveHistory:\")\n",
    "objectiveHistory.foreach(loss => println(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.- Obtain the metrics useful to judge performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binarySummary: org.apache.spark.ml.classification.BinaryLogisticRegressionSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@396171a8\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We cast the summary to a BinaryLogisticRegressionSummary since the problem is a binary classification problem.\n",
    "val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.- Obtain the receiver-operating characteristic as a dataframe and areaUnderROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|0.017543859649122806|\n",
      "|0.0| 0.03508771929824561|\n",
      "|0.0| 0.05263157894736842|\n",
      "|0.0| 0.07017543859649122|\n",
      "|0.0| 0.08771929824561403|\n",
      "|0.0| 0.10526315789473684|\n",
      "|0.0| 0.12280701754385964|\n",
      "|0.0| 0.14035087719298245|\n",
      "|0.0| 0.15789473684210525|\n",
      "|0.0| 0.17543859649122806|\n",
      "|0.0| 0.19298245614035087|\n",
      "|0.0| 0.21052631578947367|\n",
      "|0.0| 0.22807017543859648|\n",
      "|0.0| 0.24561403508771928|\n",
      "|0.0|  0.2631578947368421|\n",
      "|0.0|  0.2807017543859649|\n",
      "|0.0|  0.2982456140350877|\n",
      "|0.0|  0.3157894736842105|\n",
      "|0.0|  0.3333333333333333|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "areaUnderROC: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "roc: org.apache.spark.sql.DataFrame = [FPR: double, TPR: double]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val roc = binarySummary.roc\n",
    "roc.show()\n",
    "println(s\"areaUnderROC: ${binarySummary.areaUnderROC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.- Set the model threshold to maximize F-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fMeasure: org.apache.spark.sql.DataFrame = [threshold: double, F-Measure: double]\n",
       "maxFMeasure: Double = 1.0\n",
       "bestThreshold: Double = 0.5585022394278357\n",
       "res8: lrModel.type = LogisticRegressionModel: uid = logreg_84e198909871, numClasses = 2, numFeatures = 692\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fMeasure = binarySummary.fMeasureByThreshold\n",
    "val maxFMeasure = fMeasure.select(max(\"F-Measure\")).head().getDouble(0)\n",
    "val bestThreshold = fMeasure.where($\"F-Measure\" === maxFMeasure)\n",
    "  .select(\"threshold\").head().getDouble(0)\n",
    "\n",
    "lrModel.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
